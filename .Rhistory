data <- read.delim("jubo.txt")
View(data)
head(data)
library(keras)
tokenizer <- text_tokenizer(num_words = 500)
tokenizer %>% fit_text_tokenizer(data)
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
embedding_size <- 128  # Dimension of the embedding vector.
skip_window <- 5       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
embedding <- layer_embedding(
input_dim = tokenizer$num_words + 1,
output_dim = embedding_size,
input_length = 1,
name = "embedding"
)
target_vector <- input_target %>%
embedding() %>%
layer_flatten()
context_vector <- input_context %>%
embedding() %>%
layer_flatten()
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
model %>%
fit_generator(
skipgrams_generator(data, tokenizer, skip_window, negative_samples),
steps_per_epoch = 10000, epochs = 5
)
class(data)
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(data)
model %>%
fit_generator(
skipgrams_generator(data, tokenizer, skip_window, negative_samples),
steps_per_epoch = 10000, epochs = 5
)
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(data)
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
embedding_size <- 128  # Dimension of the embedding vector.
skip_window <- 5       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
embedding <- layer_embedding(
input_dim = tokenizer$num_words + 1,
output_dim = embedding_size,
input_length = 1,
name = "embedding"
)
target_vector <- input_target %>%
embedding() %>%
layer_flatten()
context_vector <- input_context %>%
embedding() %>%
layer_flatten()
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
model %>%
fit_generator(
skipgrams_generator(data, tokenizer, skip_window, negative_samples),
steps_per_epoch = 10000, epochs = 5
)
data <- read.delim("jubo.txt")
class(data)
head(data)
library(keras)
tokenizer <- text_tokenizer(num_words = 10)
tokenizer %>% fit_text_tokenizer(data)
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
embedding_size <- 128  # Dimension of the embedding vector.
skip_window <- 5       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
embedding <- layer_embedding(
input_dim = tokenizer$num_words + 1,
output_dim = embedding_size,
input_length = 1,
name = "embedding"
)
target_vector <- input_target %>%
embedding() %>%
layer_flatten()
context_vector <- input_context %>%
embedding() %>%
layer_flatten()
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
model %>%
fit_generator(
skipgrams_generator(data, tokenizer, skip_window, negative_samples),
steps_per_epoch = 10000, epochs = 5
)
data <- read.delim("jubo.txt")
data <- read.delim("jubo.txt")
corpus <- corpus(VectorSource(data))
corpus <- Corpus(VectorSource(data))
install.packages("tm")
library(tm)
corpus <- Corpus(VectorSource(data))
data
corpus
inspect(corpus[2])
inspect(corpus[1])
data <- readLines("jubo.text")
data <- readLines("jubo.txt")
data <- readLines("jubo.txt")
corpus <- Corpus(VectorSource(data))
inspect(corpus[12])
inspect(corpus[8])
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords("english"))
cleanset <- tm_map(corpus, stripWhitespace)
cleanset <- tm_map(corpus, PlainTextDocument)
View(cleanset)
cleanset[5]
cleanset[2]
inspect(cleanset[2])
inspect(cleanset[8])
data <- readLines("jubo.txt")
corpus <- Corpus(VectorSource(data))
inspect(corpus[8])
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords("english"))
cleanset <- tm_map(corpus, stripWhitespace)
cleanset <- tm_map(corpus, PlainTextDocument)
cleanset <- tm_map(corpus, stripWhitespace)
cleanset[12]
inspect(cleanset[12])
dtm <- TermDocumentMatrix(cleanset, control = list(minwordLength= c(1, Inf)))
findFreqTerms(dtm, lowfreq = 3)
install.packages("wordcloud")
library(ggplot2)
library(wordcloud)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=10)
barplot(termFrrequency, las = 2, col = rainbow(20))
findFreqTerms(dtm, lowfreq = 3)
cleanset <- tm_map(corpus, removeWords, stopwords("english"))
cleanset <- tm_map(corpus, stripWhitespace)
inspect(cleanset[12])
dtm <- TermDocumentMatrix(cleanset, control = list(minwordLength= c(1, Inf)))
findFreqTerms(dtm, lowfreq = 2)
library(ggplot2)
library(wordcloud)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=10)
barplot(termFrrequency, las = 2, col = rainbow(20))
stopwords("english")
data <- readLines("jubo.txt")
corpus <- Corpus(VectorSource(data))
View(corpus)
corpus <- tm_map(corpus, tolower)
inspect(corpus[1])
corpus <- tm_map(corpus, removePunctuation)
> inspect(corpus[1])
inspect(corpus[1])
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
inspect(corpus[1])
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus[1])
dtm <- TermDocumentMatrix(cleanset, control = list(minwordLength= c(1, Inf)))
dtm <- TermDocumentMatrix(corpus, control = list(minwordLength= c(1, Inf)))
findFreqTerms(dtm, lowfreq = 2)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=10)
barplot(termFrrequency, las = 2, col = rainbow(20))
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
m <- as.matrix(dtm)
wordFeq <- sort(rowSums(m), decreasing = T)
set.seed(525)
graylevels <- gray( (wordFeq+10) / max(wordFeq+10))
m <- as.matrix(dtm)
wordFeq <- sort(rowSums(m), decreasing = T)
set.seed(525)
graylevels <- gray( (wordFeq+10) / max(wordFeq+10))
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 5, random.order = F, colors = graylevels)
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
warnings()
set.seed(525)
graylevels <- gray( (wordFeq+10) / max(wordFeq+10))
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
set.seed(525)
graylevels <- gray( (wordFeq+10) / max(wordFeq)+10)
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 5, random.order = F, colors = rainbow(20))
graylevels <- gray( (wordFeq+10) / (max(wordFeq) + 10) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
graylevels <- gray( (wordFeq+5) / (max(wordFeq) + 5) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
set.seed(525)
graylevels <- gray( (wordFeq+50) / (max(wordFeq) + 50) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
library(ggplot2)
library(wordcloud)
set.seed(525)
graylevels <- gray( (wordFeq+50) / (max(wordFeq) + 50) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
m <- as.matrix(dtm)
wordFeq <- sort(rowSums(m), decreasing = T)
set.seed(525)
graylevels <- gray( (wordFeq+50) / (max(wordFeq) + 50) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
graylevels <- gray( (wordFeq+1) / (max(wordFeq) + 1) )
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq =2,random.order = F, colors = graylevels)
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 5, random.order = F, colors = rainbow(20))
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 5, random.order = T, colors = rainbow(20))
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 5, random.order = F, colors = rainbow(20))
wordcloud(words = names(wordFeq), freq = wordFeq,
min.freq = 2, random.order = F, colors = rainbow(20))
wordcloud()
?wordcloud
wordcloud(words = names(wordFeq), freq = wordFeq,scale=c(4,.5),
min.freq =2,random.order = F, colors = graylevels)
library(ggplot2)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
dtm <- TermDocumentMatrix(corpus)
library(tm)
dtm <- TermDocumentMatrix(corpus)
findFreqTerms(dtm, lowfreq = 2)
library(ggplot2)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
et.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= F, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=0.05,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=1,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 50,
max.words= 100, random.order=FALSE, rot.per=1,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 50,
max.words= 100, random.order=FALSE, rot.per=.3,
colors=brewer.pal(8, "Dark2"))
warnings()
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 60,
max.words= 100, random.order=FALSE, rot.per=.3,
colors=brewer.pal(8, "Dark2"))
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
library(tm)
data <- readLines("jubo.txt")
data <- readLines("jubo.txt")
corpus <- Corpus(VectorSource(data))
inspect(corpus[8])
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
dtm <- TermDocumentMatrix(corpus)
findFreqTerms(dtm, lowfreq = 2)
library(ggplot2)
library(wordcloud)
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 60,
max.words= 100, random.order=FALSE, rot.per=.3,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words= 100, random.order=FALSE, rot.per=.3,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words= 100, random.order=FALSE, rot.per=.3,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words= 500, random.order=FALSE, rot.per=.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words= 500, random.order=FALSE, rot.per=.2,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=.2,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per=.2,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
warnings()
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, scale=c(4,.5), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, scale=c(2,.5), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, scale=c(2,.1), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 100, random.order=FALSE, scale=c(1,.5), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 200, random.order=FALSE, scale=c(2,.8), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 200, random.order=FALSE, scale=c(1,.8), rot.per= 0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 200, random.order=FALSE, scale=c(1,.8), rot.per= 0.3,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words= 200, random.order=FALSE, scale=c(1,.5), rot.per= 0.3,
colors=brewer.pal(8, "Dark2"))
termFrrequency <- rowSums(as.matrix(dtm))
termFrrequency <- subset(termFrrequency, termFrrequency >=5)
barplot(termFrrequency, las = 2, col = rainbow(20))
